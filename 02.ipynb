{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b05f6f55-d5a9-431a-a065-448d990861e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2927f721-f12a-4c29-93ca-1480c90b202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e4a843-e5b3-41d9-9efc-783f20e4803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244ba6e8-f9a5-4bae-8dcf-f7e11eac8f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039eb236-bf68-46d1-a814-4f3a621553cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d2ce34b-6547-4838-aa07-8fc9e397e0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "701f17bd-1855-45de-9537-4c191583afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b27197-16dd-4bd6-b892-f790b336faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da42b9ca-e668-4ec3-8cf3-a7b7b8e91d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '--': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'Ah': 12, 'Among': 13, 'And': 14, 'Are': 15, 'Arrt': 16, 'As': 17, 'At': 18, 'Be': 19, 'Begin': 20, 'Burlington': 21, 'But': 22, 'By': 23, 'Carlo': 24, 'Carlo;': 25, 'Chicago': 26, 'Claude': 27, 'Come': 28, 'Croft': 29, 'Destroyed': 30, 'Devonshire': 31, 'Don': 32, 'Dubarry': 33, 'Emperors': 34, 'Florence': 35, 'For': 36, 'Gallery': 37, 'Gideon': 38, 'Gisburn': 39, 'Gisburns': 40, 'Grafton': 41, 'Greek': 42, 'Grindle': 43, 'Grindle:': 44, 'Grindles': 45, 'HAD': 46, 'Had': 47, 'Hang': 48, 'Has': 49, 'He': 50, 'Her': 51, 'Hermia': 52, 'His': 53, 'How': 54, 'I': 55, 'If': 56, 'In': 57, 'It': 58, 'Jack': 59, 'Jove': 60, 'Just': 61, 'Lord': 62, 'Made': 63, 'Miss': 64, 'Money': 65, 'Monte': 66, 'Moon-dancers': 67, 'Mr': 68, 'Mrs': 69, 'My': 70, 'Never': 71, 'No': 72, 'Now': 73, 'Nutley': 74, 'Of': 75, 'Oh': 76, 'On': 77, 'Once': 78, 'Only': 79, 'Or': 80, 'Perhaps': 81, 'Poor': 82, 'Professional': 83, 'Renaissance': 84, 'Rickham': 85, 'Rickham;': 86, 'Riviera': 87, 'Rome': 88, 'Russian': 89, 'Sevres': 90, 'She': 91, 'Stroud': 92, 'Strouds': 93, 'Suddenly': 94, 'That': 95, 'The': 96, 'Then': 97, 'There': 98, 'There:': 99, 'They': 100, 'This': 101, 'Those': 102, 'Though': 103, 'Thwing': 104, 'Thwings': 105, 'To': 106, 'Usually': 107, 'Venetian': 108, 'Victor': 109, 'Was': 110, 'We': 111, 'Well': 112, 'What': 113, 'When': 114, 'Why': 115, 'Yes': 116, 'You': 117, '_': 118, 'a': 119, 'abdication': 120, 'able': 121, 'about': 122, 'about;': 123, 'above': 124, 'abruptly': 125, 'absolute': 126, 'absorbed': 127, 'absurdity': 128, 'academic': 129, 'accuse': 130, 'accustomed': 131, 'across': 132, 'activity': 133, 'add': 134, 'added': 135, 'admirers': 136, 'adopted': 137, 'adulation': 138, 'advance': 139, 'aesthetic': 140, 'affect': 141, 'afraid': 142, 'after': 143, 'afterward': 144, 'again': 145, 'ago': 146, 'ah': 147, 'air': 148, 'alive': 149, 'all': 150, 'almost': 151, 'alone': 152, 'along': 153, 'always': 154, 'am': 155, 'amazement': 156, 'amid': 157, 'among': 158, 'amplest': 159, 'amusing': 160, 'an': 161, 'and': 162, 'another': 163, 'answer': 164, 'answered': 165, 'any': 166, 'anything': 167, 'anywhere': 168, 'apparent': 169, 'apparently': 170, 'appearance': 171, 'appeared': 172, 'appointed': 173, 'are': 174, 'arm': 175, 'arm-chair': 176, 'arm-chairs': 177, 'arms': 178, 'art': 179, 'articles': 180, 'artist': 181, 'as': 182, 'aside': 183, 'asked': 184, 'at': 185, 'atmosphere': 186, 'atom': 187, 'attack': 188, 'attention': 189, 'attention;': 190, 'attitude': 191, 'audacities': 192, 'away': 193, 'awful': 194, 'axioms': 195, 'azaleas': 196, 'back': 197, 'background': 198, 'balance': 199, 'balancing': 200, 'balustraded': 201, 'basking': 202, 'bath-rooms': 203, 'be': 204, 'beaming': 205, 'bean-stalk': 206, 'bear': 207, 'beard': 208, 'beauty': 209, 'became': 210, 'because': 211, 'becoming': 212, 'bed': 213, 'been': 214, 'before': 215, 'began': 216, 'begun': 217, 'behind': 218, 'being': 219, 'believed': 220, 'beneath': 221, 'bespoke': 222, 'better': 223, 'better;': 224, 'between': 225, 'big': 226, 'bits': 227, 'bitterness': 228, 'blocked': 229, 'born': 230, 'borne': 231, 'boudoir': 232, 'bravura': 233, 'break': 234, 'breaking': 235, 'breathing': 236, 'bric-a-brac': 237, 'briefly': 238, 'brings': 239, 'bronzes': 240, 'brought': 241, 'brown': 242, 'brush': 243, 'bull': 244, 'business': 245, 'but': 246, 'buying': 247, 'by': 248, 'called': 249, 'came': 250, 'can': 251, 'canvas': 252, 'canvases': 253, 'cards': 254, 'care': 255, 'career': 256, 'caught': 257, 'central': 258, 'chair': 259, 'chap': 260, 'characteristic': 261, 'charming': 262, 'cheap': 263, 'check': 264, 'cheeks': 265, 'chest': 266, 'chimney-piece': 267, 'chucked': 268, 'cigar': 269, 'cigarette': 270, 'cigars': 271, 'circulation': 272, 'circumstance': 273, 'circus-clown': 274, 'claimed': 275, 'clasping': 276, 'clear': 277, 'cleverer': 278, 'close': 279, 'clue': 280, 'coat': 281, 'collapsed': 282, 'colour': 283, 'come': 284, 'comfortable': 285, 'coming': 286, 'companion': 287, 'compared': 288, 'complex': 289, 'confident': 290, 'congesting': 291, 'conjugal': 292, 'constraint': 293, 'consummate': 294, 'contended': 295, 'continued': 296, 'corner': 297, 'corrected': 298, 'could': 299, 'couldn': 300, 'count': 301, 'countenance': 302, 'couple': 303, 'course': 304, 'covered': 305, 'craft': 306, 'cried': 307, 'crossed': 308, 'crowned': 309, 'crumbled': 310, 'cry': 311, 'cured': 312, 'curiosity': 313, 'curious': 314, 'current': 315, 'curtains': 316, 'd': 317, 'dabble': 318, 'damask': 319, 'dark': 320, 'dashed': 321, 'day': 322, 'days': 323, 'dead': 324, 'deadening': 325, 'dear': 326, 'deep': 327, 'deerhound': 328, 'degree': 329, 'delicate': 330, 'demand': 331, 'denied': 332, 'deploring': 333, 'deprecating': 334, 'deprecatingly': 335, 'desire': 336, 'destroyed': 337, 'destruction': 338, 'desultory': 339, 'detail': 340, 'diagnosis': 341, 'did': 342, 'didn': 343, 'died': 344, 'dim': 345, 'dimmest': 346, 'dingy': 347, 'dining-room': 348, 'disarming': 349, 'discovery;': 350, 'discrimination': 351, 'discussion': 352, 'disdain': 353, 'disdained': 354, 'disease': 355, 'disguised': 356, 'display': 357, 'dissatisfied': 358, 'distinguished': 359, 'distract': 360, 'divert': 361, 'do': 362, 'doesn': 363, 'doing': 364, 'domestic': 365, 'don': 366, 'done': 367, 'donkey': 368, 'down': 369, 'dozen': 370, 'dragged': 371, 'drawing-room': 372, 'drawing-rooms': 373, 'drawn': 374, 'dress-closets': 375, 'drew': 376, 'dropped': 377, 'each': 378, 'earth': 379, 'ease': 380, 'easel': 381, 'easy': 382, 'echoed': 383, 'economy': 384, 'effect': 385, 'effects': 386, 'efforts': 387, 'egregious': 388, 'eighteenth-century': 389, 'elbow': 390, 'elegant': 391, 'else': 392, 'embarrassed': 393, 'enabled': 394, 'end': 395, 'endless': 396, 'enjoy': 397, 'enlightenment:': 398, 'enough': 399, 'ensuing': 400, 'equally': 401, 'equanimity': 402, 'escape': 403, 'established': 404, 'etching': 405, 'even': 406, 'event': 407, 'ever': 408, 'everlasting': 409, 'every': 410, 'exasperated': 411, 'except': 412, 'excuse': 413, 'excusing': 414, 'existed': 415, 'expected': 416, 'exquisite': 417, 'exquisitely': 418, 'extenuation': 419, 'exterminating': 420, 'extracting': 421, 'eye': 422, 'eyebrows': 423, 'eyes': 424, 'eyes:': 425, 'face': 426, 'faces': 427, 'fact': 428, 'faded': 429, 'failed': 430, 'failure': 431, 'fair': 432, 'faith': 433, 'false': 434, 'familiar': 435, 'famille-verte': 436, 'fancy': 437, 'fashionable': 438, 'fate': 439, 'feather': 440, 'feet': 441, 'fell': 442, 'fellow': 443, 'felt': 444, 'few': 445, 'fewer': 446, 'finality': 447, 'find': 448, 'fingers': 449, 'first': 450, 'fit': 451, 'fitting': 452, 'five': 453, 'flash': 454, 'flashed': 455, 'florid': 456, 'flowers': 457, 'fluently': 458, 'flung': 459, 'follow': 460, 'followed': 461, 'fond': 462, 'footstep': 463, 'for': 464, 'forced': 465, 'forcing': 466, 'forehead': 467, 'foreign': 468, 'foreseen': 469, 'forgive': 470, 'forgotten': 471, 'form': 472, 'formed': 473, 'forming': 474, 'forward': 475, 'fostered': 476, 'found': 477, 'foundations': 478, 'fragment': 479, 'fragments': 480, 'frame': 481, 'frames': 482, 'frequently': 483, 'friend': 484, 'from': 485, 'full': 486, 'fullest': 487, 'furiously': 488, 'furrowed': 489, 'garlanded': 490, 'garlands': 491, 'gave': 492, 'genial': 493, 'genius': 494, 'gesture': 495, 'get': 496, 'getting': 497, 'give': 498, 'given': 499, 'glad': 500, 'glanced': 501, 'glimpse': 502, 'gloried': 503, 'glory': 504, 'go': 505, 'going': 506, 'gone': 507, 'good': 508, 'good-breeding': 509, 'good-humoured': 510, 'got': 511, 'grace': 512, 'gradually': 513, 'gray': 514, 'grayish': 515, 'great': 516, 'greatest': 517, 'greatness': 518, 'grew': 519, 'groping': 520, 'growing': 521, 'had': 522, 'hadn': 523, 'hair': 524, 'half': 525, 'half-light': 526, 'half-mechanically': 527, 'hall': 528, 'hand': 529, 'hands': 530, 'handsome': 531, 'hanging': 532, 'happen': 533, 'happened': 534, 'hard': 535, 'hardly': 536, 'has': 537, 'have': 538, 'haven': 539, 'having': 540, 'he': 541, 'head': 542, 'hear': 543, 'heard': 544, 'heart': 545, 'height': 546, 'her': 547, 'here': 548, 'here;': 549, 'hermit': 550, 'herself': 551, 'hesitations': 552, 'hide': 553, 'high': 554, 'him': 555, 'him:': 556, 'himself': 557, 'hint': 558, 'his': 559, 'history': 560, 'holding': 561, 'home': 562, 'honour': 563, 'hooded': 564, 'hostess': 565, 'hostess:': 566, 'hot-house': 567, 'hour': 568, 'hours': 569, 'house': 570, 'how': 571, 'hung': 572, 'husband': 573, 'idea': 574, 'idle': 575, 'idling': 576, 'if': 577, 'immediately': 578, 'in': 579, 'incense': 580, 'indifferent;': 581, 'inevitable': 582, 'inevitably': 583, 'inflexible': 584, 'insensible': 585, 'insignificant': 586, 'instinctively': 587, 'instructive': 588, 'interesting': 589, 'into': 590, 'ironic': 591, 'irony': 592, 'irrelevance': 593, 'irrevocable': 594, 'is': 595, 'it': 596, 'it;': 597, 'its': 598, 'itself': 599, 'jardiniere': 600, 'jealousy': 601, 'just': 602, 'keep': 603, 'kept': 604, 'kind': 605, 'knees': 606, 'knew': 607, 'know': 608, 'know;': 609, 'known': 610, 'laid': 611, 'lair': 612, 'landing': 613, 'language': 614, 'last': 615, 'late': 616, 'later': 617, 'latter': 618, 'laugh': 619, 'laugh:': 620, 'laughed': 621, 'lay': 622, 'leading': 623, 'lean': 624, 'learned': 625, 'least': 626, 'leathery:': 627, 'leave': 628, 'led': 629, 'left': 630, 'leisure': 631, 'lends': 632, 'lent': 633, 'let': 634, 'lies': 635, 'life': 636, 'life-likeness': 637, 'lift': 638, 'lifted': 639, 'light': 640, 'lightly;': 641, 'like': 642, 'liked': 643, 'line': 644, 'lines': 645, 'lingered': 646, 'lips': 647, 'lit': 648, 'little': 649, 'little:': 650, 'live': 651, 'll': 652, 'loathing': 653, 'long': 654, 'longed': 655, 'longer': 656, 'look': 657, 'looked': 658, 'looking': 659, 'lose': 660, 'loss': 661, 'lounging': 662, 'lovely': 663, 'lucky': 664, 'lump': 665, 'luncheon-table': 666, 'luxury': 667, 'lying': 668, 'made': 669, 'make': 670, 'man': 671, 'manage': 672, 'managed': 673, 'mantel-piece': 674, 'marble': 675, 'married': 676, 'may': 677, 'me': 678, 'meant': 679, 'mediocrity': 680, 'medium': 681, 'mentioned': 682, 'mere': 683, 'merely': 684, 'met': 685, 'might': 686, 'mighty': 687, 'millionaire': 688, 'mine': 689, 'mine:': 690, 'minute': 691, 'minutes': 692, 'mirrors': 693, 'modest': 694, 'modesty': 695, 'moment': 696, 'money': 697, 'monumental': 698, 'mood': 699, 'morbidly': 700, 'more': 701, 'most': 702, 'mourn': 703, 'mourned': 704, 'moustache': 705, 'moved': 706, 'much': 707, 'muddling;': 708, 'multiplied': 709, 'murmur': 710, 'muscles': 711, 'must': 712, 'my': 713, 'myself': 714, 'mysterious': 715, 'naive': 716, 'near': 717, 'nearly': 718, 'negatived': 719, 'nervous': 720, 'nervousness;': 721, 'neutral': 722, 'never': 723, 'next': 724, 'no': 725, 'none': 726, 'not': 727, 'note': 728, 'nothing': 729, 'now': 730, 'nymphs': 731, 'oak': 732, 'obituary': 733, 'object': 734, 'objects': 735, 'occurred': 736, 'oddly': 737, 'of': 738, 'off': 739, 'often': 740, 'oh': 741, 'old': 742, 'on': 743, 'once': 744, 'one': 745, 'ones': 746, 'only': 747, 'onto': 748, 'open': 749, 'or': 750, 'other': 751, 'our': 752, 'ourselves': 753, 'out': 754, 'out:': 755, 'outline': 756, 'oval': 757, 'over': 758, 'own': 759, 'packed': 760, 'paid': 761, 'paint': 762, 'painted': 763, 'painted;': 764, 'painter': 765, 'painting': 766, 'painting;': 767, 'pale': 768, 'paled': 769, 'palm-trees;': 770, 'panel': 771, 'panelling': 772, 'pardonable': 773, 'pardoned': 774, 'part': 775, 'passages': 776, 'passing': 777, 'past': 778, 'pastels': 779, 'pathos': 780, 'patient': 781, 'people': 782, 'perceptible': 783, 'perfect': 784, 'persistence': 785, 'persuasively': 786, 'phrase': 787, 'picture': 788, 'pictures': 789, 'pines': 790, 'pink': 791, 'place': 792, 'placed': 793, 'plain': 794, 'platitudes': 795, 'pleased': 796, 'pockets': 797, 'point': 798, 'poised': 799, 'poor': 800, 'portrait': 801, 'posing': 802, 'possessed': 803, 'poverty': 804, 'predicted': 805, 'preliminary': 806, 'presenting': 807, 'prestidigitation': 808, 'pretty': 809, 'previous': 810, 'price': 811, 'pride': 812, 'pride:': 813, 'princely': 814, 'prism': 815, 'problem': 816, 'proclaiming': 817, 'prodigious': 818, 'profusion': 819, 'protest': 820, 'prove': 821, 'public': 822, 'purblind': 823, 'purely': 824, 'pushed': 825, 'put': 826, 'qualities': 827, 'quality': 828, 'queerly': 829, 'question': 830, 'question:': 831, 'quickly': 832, 'quietly': 833, 'quite': 834, 'quote': 835, 'rain': 836, 'raised': 837, 'random': 838, 'rather': 839, 're': 840, 'real': 841, 'really': 842, 'reared': 843, 'reason': 844, 'reassurance': 845, 'recovering': 846, 'recreated': 847, 'reflected': 848, 'reflection': 849, 'regrets': 850, 'relatively': 851, 'remained': 852, 'remember': 853, 'reminded': 854, 'repeating': 855, 'represented': 856, 'reproduction': 857, 'resented': 858, 'resolve': 859, 'resources': 860, 'rest': 861, 'rich': 862, 'rich;': 863, 'ridiculous': 864, 'robbed': 865, 'romantic': 866, 'room': 867, 'rose': 868, 'rs': 869, 'rule': 870, 'run': 871, 's': 872, 'said': 873, 'said:': 874, 'same': 875, 'satisfaction': 876, 'satisfaction:': 877, 'savour': 878, 'saw': 879, 'say': 880, 'say:': 881, 'saying': 882, 'says': 883, 'scorn': 884, 'scornful': 885, 'secret': 886, 'see': 887, 'seemed': 888, 'seen': 889, 'self-confident': 890, 'send': 891, 'sensation': 892, 'sensitive': 893, 'sent': 894, 'serious': 895, 'set': 896, 'sex': 897, 'shade': 898, 'shaking': 899, 'shall': 900, 'she': 901, 'shirked': 902, 'short': 903, 'should': 904, 'shoulder': 905, 'shoulders': 906, 'show': 907, 'showed': 908, 'showy': 909, 'shrug': 910, 'shrugged': 911, 'sight': 912, 'sign': 913, 'silent;': 914, 'silver': 915, 'similar': 916, 'simpleton': 917, 'simplifications': 918, 'simply': 919, 'since': 920, 'single': 921, 'sitter': 922, 'sitters': 923, 'sketch': 924, 'skill': 925, 'slight': 926, 'slightly': 927, 'slowly:': 928, 'small': 929, 'smile': 930, 'smiling': 931, 'sneer': 932, 'so': 933, 'solace': 934, 'some': 935, 'somebody': 936, 'something': 937, 'spacious': 938, 'spaniel': 939, 'speaking-tubes': 940, 'speculations;': 941, 'spite': 942, 'splash': 943, 'square': 944, 'stairs': 945, 'stammer': 946, 'stand': 947, 'standing': 948, 'started': 949, 'stay': 950, 'still': 951, 'stocked': 952, 'stood': 953, 'stopped': 954, 'stopping': 955, 'straddling': 956, 'straight': 957, 'strain': 958, 'straining': 959, 'strange': 960, 'straw': 961, 'stream': 962, 'stroke': 963, 'strokes': 964, 'strolled': 965, 'strongest': 966, 'strongly': 967, 'struck': 968, 'studio': 969, 'stuff': 970, 'subject': 971, 'substantial': 972, 'suburban': 973, 'such': 974, 'suddenly': 975, 'suffered': 976, 'sugar': 977, 'suggested': 978, 'sunburn': 979, 'sunburnt': 980, 'sunlit': 981, 'superb': 982, 'sure': 983, 'surest': 984, 'surface': 985, 'surprise': 986, 'surprised': 987, 'surrounded': 988, 'suspected': 989, 'sweetly': 990, 'sweetness': 991, 'swelling': 992, 'swept': 993, 'swum': 994, 't': 995, 'table': 996, 'take': 997, 'taken': 998, 'talking': 999, 'tea': 1000, 'tears': 1001, 'technicalities': 1002, 'technique': 1003, 'tell': 1004, 'tells': 1005, 'tempting': 1006, 'terra-cotta': 1007, 'terrace': 1008, 'terraces': 1009, 'terribly': 1010, 'than': 1011, 'that': 1012, 'the': 1013, 'their': 1014, 'them': 1015, 'then': 1016, 'there': 1017, 'therefore': 1018, 'they': 1019, 'thin': 1020, 'thing': 1021, 'things': 1022, 'think': 1023, 'this': 1024, 'thither': 1025, 'those': 1026, 'though': 1027, 'thought': 1028, 'thought:': 1029, 'three': 1030, 'threshold': 1031, 'threw': 1032, 'through': 1033, 'throwing': 1034, 'tie': 1035, 'till': 1036, 'time': 1037, 'timorously': 1038, 'tinge': 1039, 'tips': 1040, 'tired': 1041, 'to': 1042, 'told': 1043, 'tone': 1044, 'tones': 1045, 'too': 1046, 'took': 1047, 'tottering': 1048, 'touched': 1049, 'toward': 1050, 'trace': 1051, 'trade': 1052, 'transmute': 1053, 'traps': 1054, 'travelled': 1055, 'tribute': 1056, 'tributes': 1057, 'tricks': 1058, 'tried': 1059, 'trouser-presses': 1060, 'true': 1061, 'truth': 1062, 'turned': 1063, 'twenty': 1064, 'twenty-four': 1065, 'twice': 1066, 'twirling': 1067, 'unaccountable': 1068, 'uncertain': 1069, 'under': 1070, 'underlay': 1071, 'underneath': 1072, 'understand': 1073, 'unexpected': 1074, 'untouched': 1075, 'unusual': 1076, 'up': 1077, 'up-stream': 1078, 'up;': 1079, 'upon': 1080, 'upset': 1081, 'upstairs': 1082, 'us': 1083, 'used': 1084, 'usual': 1085, 'value': 1086, 'varnishing': 1087, 'vases': 1088, 've': 1089, 'veins': 1090, 'velveteen': 1091, 'very': 1092, 'villa': 1093, 'vindicated': 1094, 'virtuosity': 1095, 'vista': 1096, 'vocation': 1097, 'voice': 1098, 'wall': 1099, 'wander': 1100, 'want': 1101, 'wanted': 1102, 'wants': 1103, 'was': 1104, 'wasn': 1105, 'watched': 1106, 'watching': 1107, 'water-colour': 1108, 'waves': 1109, 'way': 1110, 'weekly': 1111, 'weeks': 1112, 'welcome': 1113, 'went': 1114, 'were': 1115, 'what': 1116, 'when': 1117, 'whenever': 1118, 'where': 1119, 'which': 1120, 'while': 1121, 'white': 1122, 'white-panelled': 1123, 'who': 1124, 'whole': 1125, 'whom': 1126, 'why': 1127, 'wide': 1128, 'widow': 1129, 'wife': 1130, 'wild': 1131, 'wincing': 1132, 'wincing;': 1133, 'window-curtains': 1134, 'wish': 1135, 'with': 1136, 'without': 1137, 'wits': 1138, 'woman': 1139, 'women': 1140, 'women:': 1141, 'won': 1142, 'wonder': 1143, 'wondered': 1144, 'word': 1145, 'work': 1146, 'working': 1147, 'worth': 1148, 'would': 1149, 'wouldn': 1150, 'year': 1151, 'years': 1152, 'yellow': 1153, 'yet': 1154, 'you': 1155, 'younger': 1156, 'your': 1157, 'yourself': 1158}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54cec90-fcab-492b-9109-3355b6f3dc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Carlo;', 25)\n",
      "('Chicago', 26)\n",
      "('Claude', 27)\n",
      "('Come', 28)\n",
      "('Croft', 29)\n",
      "('Destroyed', 30)\n",
      "('Devonshire', 31)\n",
      "('Don', 32)\n",
      "('Dubarry', 33)\n",
      "('Emperors', 34)\n",
      "('Florence', 35)\n",
      "('For', 36)\n",
      "('Gallery', 37)\n",
      "('Gideon', 38)\n",
      "('Gisburn', 39)\n",
      "('Gisburns', 40)\n",
      "('Grafton', 41)\n",
      "('Greek', 42)\n",
      "('Grindle', 43)\n",
      "('Grindle:', 44)\n",
      "('Grindles', 45)\n",
      "('HAD', 46)\n",
      "('Had', 47)\n",
      "('Hang', 48)\n",
      "('Has', 49)\n",
      "('He', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16068501-2bba-47b5-9fd2-6b860d30d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5be0ecb2-ff54-4cfd-8791-c95f92c1ecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05ffb132-902c-4cf8-92b9-fe02f42bb352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3739e472-a4f6-4672-8345-8c5f4e327393",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d274f3d-709b-491c-bc3d-18e93ba27965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a033cf81-7eef-43c5-8ef2-98c7cce213fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1156)\n",
      "('your', 1157)\n",
      "('yourself', 1158)\n",
      "('<|endoftext|>', 1159)\n",
      "('<|unk|>', 1160)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ea53f45-a7cc-4037-b7d4-54f98a7f3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int \n",
    "                        else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd83247b-2cbb-47ab-ad31-f0f83097b333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d718721-2011-4596-8939-89d3e9436a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1160,\n",
       " 5,\n",
       " 362,\n",
       " 1155,\n",
       " 642,\n",
       " 1000,\n",
       " 10,\n",
       " 1159,\n",
       " 57,\n",
       " 1013,\n",
       " 981,\n",
       " 1009,\n",
       " 738,\n",
       " 1013,\n",
       " 1160,\n",
       " 7]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bd4b66b-5be4-45ee-a408-c54df766ac22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.6.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9efce36-cd8f-44a9-a105-5996d9ec883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5d0ee3f-0bd3-42bc-9451-b0411b420ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb5f9e15-9d7a-4dae-942a-6efe94c28834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "217d9e30-59e9-4eaa-9a72-9032cbd18857",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2845204c-f5c9-4ca5-8764-b8e8f151adc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c21009f0-7e64-438a-ac2c-225fcc68b779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1aabe53e-6df4-41a3-92ae-c016984932fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.9.1+cu111\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55662560-f838-4809-b7fb-bfeeaa9a8d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41d584d3-4b40-48bf-988b-2ebdfa34e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20f88669-7b6d-42bd-9854-4b8dd2f9d90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2b7e22b-d352-488e-a8fc-f04ae4a14e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34fe468f-25d7-478f-8b92-7ab430c2c47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f06838e-f09b-42b4-824b-3aede7e08e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b8871db-37d0-4203-a186-7958d3d56745",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf9b6d11-fbd6-4e0c-970f-98c399c2251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fe35f45-6efe-4668-9901-86778d05b571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82fd4aae-a15f-4fc6-b6ec-39825828e6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98259e2c-7b4d-4f6b-8558-aa2844b42cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c0336ad-e218-431f-9ba2-21ab900c66a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[-0.8489, -1.1435, -0.4694,  ..., -0.2089,  0.1135,  0.3552],\n",
      "        [-1.0793, -0.7452,  0.1640,  ...,  1.3452, -0.2725, -0.4442],\n",
      "        [ 0.2269, -0.8094, -1.4388,  ...,  0.1820,  0.3465,  0.0368],\n",
      "        [ 0.4098,  0.5486,  1.2321,  ...,  0.4239,  0.0109, -1.0677]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "795daa42-1a6e-4225-8e55-b3088071522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115c601-007f-4ea2-aeb8-716ed008898a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a537d-b1bd-4c36-a181-68ec71d1b58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
