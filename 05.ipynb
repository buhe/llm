{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cade31c4-59c8-4ec6-be38-34bf4f2dee1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello, I am\n",
      "Encoded input text: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "tok_embeds.shape torch.Size([1, 4, 768])\n",
      "tok_embeds.shape torch.Size([1, 5, 768])\n",
      "tok_embeds.shape torch.Size([1, 6, 768])\n",
      "tok_embeds.shape torch.Size([1, 7, 768])\n",
      "tok_embeds.shape torch.Size([1, 8, 768])\n",
      "tok_embeds.shape torch.Size([1, 9, 768])\n",
      "tok_embeds.shape torch.Size([1, 10, 768])\n",
      "tok_embeds.shape torch.Size([1, 11, 768])\n",
      "tok_embeds.shape torch.Size([1, 12, 768])\n",
      "tok_embeds.shape torch.Size([1, 13, 768])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
      "         49706, 43231, 47062, 34657]])\n",
      "Output length: 14\n",
      "Output text: Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
    "# Source for \"Build a Large Language Model From Scratch\"\n",
    "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
    "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
    "#\n",
    "# This file collects all the relevant code that we covered thus far\n",
    "# throughout Chapters 2-4.\n",
    "# This file can be run as a standalone script.\n",
    "\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "#####################################\n",
    "# Chapter 2\n",
    "#####################################\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=0)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 3\n",
    "#####################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -math.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 4\n",
    "#####################################\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / math.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        print(\"tok_embeds.shape\", tok_embeds.shape)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"emb_dim\": 768,          # Embedding dimension\n",
    "        \"n_heads\": 12,           # Number of attention heads\n",
    "        \"n_layers\": 12,          # Number of layers\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": False        # Query-Key-Value bias\n",
    "    }\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.eval()  # disable dropout\n",
    "\n",
    "    start_context = \"Hello, I am\"\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "    print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "    print(\"\\nInput text:\", start_context)\n",
    "    print(\"Encoded input text:\", encoded)\n",
    "    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "    out = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=encoded_tensor,\n",
    "        max_new_tokens=10,\n",
    "        context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "    )\n",
    "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "\n",
    "    print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "    print(\"\\nOutput:\", out)\n",
    "    print(\"Output length:\", len(out[0]))\n",
    "    print(\"Output text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b952a31d-0188-4151-9bc4-9ea664027290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.8.4\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.6.0\n",
      "torch version: 1.9.1+cu111\n",
      "tensorflow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a25a123d-b95e-4956-9358-2d09e8ec2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5873cb80-fc56-46ff-992e-ef44bf27a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeds.shape torch.Size([1, 4, 768])\n",
      "tok_embeds.shape torch.Size([1, 5, 768])\n",
      "tok_embeds.shape torch.Size([1, 6, 768])\n",
      "tok_embeds.shape torch.Size([1, 7, 768])\n",
      "tok_embeds.shape torch.Size([1, 8, 768])\n",
      "tok_embeds.shape torch.Size([1, 9, 768])\n",
      "tok_embeds.shape torch.Size([1, 10, 768])\n",
      "tok_embeds.shape torch.Size([1, 11, 768])\n",
      "tok_embeds.shape torch.Size([1, 12, 768])\n",
      "tok_embeds.shape torch.Size([1, 13, 768])\n",
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c1cba63-3310-4952-9491-6589b4f6281e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fcbaee10-b240-474e-bb2e-2f034e30b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [588,  428,  11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3746397-6833-424d-8da8-ff8455f0ce11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeds.shape torch.Size([2, 3, 768])\n",
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "979ce891-3775-49ad-a8bf-405736f42f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids) #预测的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cb1ee004-f437-42e2-a5a3-84b142faeb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "44f6aaca-9176-40a1-8b9e-fe7d479e056b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16657,   339, 42826])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "76c0ca9f-39c9-42bb-ab1a-721f36564322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([3.9836e-05, 1.6783e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6a1ff97d-f702-47a6-9112-f187e073dc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "97d0b2cd-4732-4fed-8e3a-e3329905ccc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3626, 6100,  345])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "db32226c-ef6a-4651-a631-9377187ad135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  588,   428, 11311])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a427de0e-8594-4219-82e4-4971878de1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " probas[0, [0, 1, 2], [3626, 6100,  345]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "30608734-1791-4378-bb57-34a8bea24b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.4541e-05)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas[0,0,3626] \n",
    "# 在没训练情况，这是选中第一个字的概率，看到得到的概率值极低，而正确应该是概率值很高，越接近1越好\n",
    "# targets 是真实小说中的文本信息\n",
    "# probas = torch.softmax(logits, dim=-1)\n",
    "# probas 是模型生成的文本概率\n",
    "# 说明，在本该正确的位置生成的文本概率却极低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8286bd6b-b269-4b18-8528-247ed00c8efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1061e-05)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas[0,1,6100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "78686ad6-6481-412e-bc5a-caf0fafbf5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4862)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0,0,3626]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2e58bbbb-b27d-470c-bcd7-b155c4f441bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a91fa2fd-3de9-4af8-a276-cd17d3fa11f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7722)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "80756747-5815-4214-9554-66b711784b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(47678.8633)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c0dfabba-ac14-466a-be3f-8b07578137b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5db2ea37-6a78-4cf2-8e08-df5f456c66d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ff9c35c-43c7-4b75-b345-97448c03b11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b508a52a-b001-476b-bc5e-a27b4179fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0033cedb-5ec7-4f08-8f88-973b0c83272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "19521b59-1758-4b8c-87ac-6475f497a74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514.4999999999999"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens * (1-train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c875a47b-6b9d-405d-94a2-5e9bfa6b354f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " GPT_CONFIG_124M[\"context_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d6716149-9d47-4858-8e62-cc08206d5c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 256,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': False}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1db47b89-f64e-4a48-9f82-5384dbabf823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f7766f55-3632-4010-b007-16d247912b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2a072af2-abd5-4e8c-b948-1e5ebea8d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "   \n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    print(\"num_batches\", num_batches)\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "06b2e323-ac0e-4444-b96a-07e17776d794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 9\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Training loss: 10.987579345703125\n",
      "Validation loss: 10.981115341186523\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "90160d03-30db-46b9-b108-f44d83c5448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous epoch\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01160985-80a5-405d-9686-6912842fbe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 1 (Step 000000): Train loss 9.820, Val loss 9.933\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 1 (Step 000005): Train loss 8.066, Val loss 8.341\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([1, 4, 768])\n",
      "tok_embeds.shape torch.Size([1, 5, 768])\n",
      "tok_embeds.shape torch.Size([1, 6, 768])\n",
      "tok_embeds.shape torch.Size([1, 7, 768])\n",
      "tok_embeds.shape torch.Size([1, 8, 768])\n",
      "tok_embeds.shape torch.Size([1, 9, 768])\n",
      "tok_embeds.shape torch.Size([1, 10, 768])\n",
      "tok_embeds.shape torch.Size([1, 11, 768])\n",
      "tok_embeds.shape torch.Size([1, 12, 768])\n",
      "tok_embeds.shape torch.Size([1, 13, 768])\n",
      "tok_embeds.shape torch.Size([1, 14, 768])\n",
      "tok_embeds.shape torch.Size([1, 15, 768])\n",
      "tok_embeds.shape torch.Size([1, 16, 768])\n",
      "tok_embeds.shape torch.Size([1, 17, 768])\n",
      "tok_embeds.shape torch.Size([1, 18, 768])\n",
      "tok_embeds.shape torch.Size([1, 19, 768])\n",
      "tok_embeds.shape torch.Size([1, 20, 768])\n",
      "tok_embeds.shape torch.Size([1, 21, 768])\n",
      "tok_embeds.shape torch.Size([1, 22, 768])\n",
      "tok_embeds.shape torch.Size([1, 23, 768])\n",
      "tok_embeds.shape torch.Size([1, 24, 768])\n",
      "tok_embeds.shape torch.Size([1, 25, 768])\n",
      "tok_embeds.shape torch.Size([1, 26, 768])\n",
      "tok_embeds.shape torch.Size([1, 27, 768])\n",
      "tok_embeds.shape torch.Size([1, 28, 768])\n",
      "tok_embeds.shape torch.Size([1, 29, 768])\n",
      "tok_embeds.shape torch.Size([1, 30, 768])\n",
      "tok_embeds.shape torch.Size([1, 31, 768])\n",
      "tok_embeds.shape torch.Size([1, 32, 768])\n",
      "tok_embeds.shape torch.Size([1, 33, 768])\n",
      "tok_embeds.shape torch.Size([1, 34, 768])\n",
      "tok_embeds.shape torch.Size([1, 35, 768])\n",
      "tok_embeds.shape torch.Size([1, 36, 768])\n",
      "tok_embeds.shape torch.Size([1, 37, 768])\n",
      "tok_embeds.shape torch.Size([1, 38, 768])\n",
      "tok_embeds.shape torch.Size([1, 39, 768])\n",
      "tok_embeds.shape torch.Size([1, 40, 768])\n",
      "tok_embeds.shape torch.Size([1, 41, 768])\n",
      "tok_embeds.shape torch.Size([1, 42, 768])\n",
      "tok_embeds.shape torch.Size([1, 43, 768])\n",
      "tok_embeds.shape torch.Size([1, 44, 768])\n",
      "tok_embeds.shape torch.Size([1, 45, 768])\n",
      "tok_embeds.shape torch.Size([1, 46, 768])\n",
      "tok_embeds.shape torch.Size([1, 47, 768])\n",
      "tok_embeds.shape torch.Size([1, 48, 768])\n",
      "tok_embeds.shape torch.Size([1, 49, 768])\n",
      "tok_embeds.shape torch.Size([1, 50, 768])\n",
      "tok_embeds.shape torch.Size([1, 51, 768])\n",
      "tok_embeds.shape torch.Size([1, 52, 768])\n",
      "tok_embeds.shape torch.Size([1, 53, 768])\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 2 (Step 000010): Train loss 6.621, Val loss 7.052\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 2 (Step 000015): Train loss 6.048, Val loss 6.601\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([1, 4, 768])\n",
      "tok_embeds.shape torch.Size([1, 5, 768])\n",
      "tok_embeds.shape torch.Size([1, 6, 768])\n",
      "tok_embeds.shape torch.Size([1, 7, 768])\n",
      "tok_embeds.shape torch.Size([1, 8, 768])\n",
      "tok_embeds.shape torch.Size([1, 9, 768])\n",
      "tok_embeds.shape torch.Size([1, 10, 768])\n",
      "tok_embeds.shape torch.Size([1, 11, 768])\n",
      "tok_embeds.shape torch.Size([1, 12, 768])\n",
      "tok_embeds.shape torch.Size([1, 13, 768])\n",
      "tok_embeds.shape torch.Size([1, 14, 768])\n",
      "tok_embeds.shape torch.Size([1, 15, 768])\n",
      "tok_embeds.shape torch.Size([1, 16, 768])\n",
      "tok_embeds.shape torch.Size([1, 17, 768])\n",
      "tok_embeds.shape torch.Size([1, 18, 768])\n",
      "tok_embeds.shape torch.Size([1, 19, 768])\n",
      "tok_embeds.shape torch.Size([1, 20, 768])\n",
      "tok_embeds.shape torch.Size([1, 21, 768])\n",
      "tok_embeds.shape torch.Size([1, 22, 768])\n",
      "tok_embeds.shape torch.Size([1, 23, 768])\n",
      "tok_embeds.shape torch.Size([1, 24, 768])\n",
      "tok_embeds.shape torch.Size([1, 25, 768])\n",
      "tok_embeds.shape torch.Size([1, 26, 768])\n",
      "tok_embeds.shape torch.Size([1, 27, 768])\n",
      "tok_embeds.shape torch.Size([1, 28, 768])\n",
      "tok_embeds.shape torch.Size([1, 29, 768])\n",
      "tok_embeds.shape torch.Size([1, 30, 768])\n",
      "tok_embeds.shape torch.Size([1, 31, 768])\n",
      "tok_embeds.shape torch.Size([1, 32, 768])\n",
      "tok_embeds.shape torch.Size([1, 33, 768])\n",
      "tok_embeds.shape torch.Size([1, 34, 768])\n",
      "tok_embeds.shape torch.Size([1, 35, 768])\n",
      "tok_embeds.shape torch.Size([1, 36, 768])\n",
      "tok_embeds.shape torch.Size([1, 37, 768])\n",
      "tok_embeds.shape torch.Size([1, 38, 768])\n",
      "tok_embeds.shape torch.Size([1, 39, 768])\n",
      "tok_embeds.shape torch.Size([1, 40, 768])\n",
      "tok_embeds.shape torch.Size([1, 41, 768])\n",
      "tok_embeds.shape torch.Size([1, 42, 768])\n",
      "tok_embeds.shape torch.Size([1, 43, 768])\n",
      "tok_embeds.shape torch.Size([1, 44, 768])\n",
      "tok_embeds.shape torch.Size([1, 45, 768])\n",
      "tok_embeds.shape torch.Size([1, 46, 768])\n",
      "tok_embeds.shape torch.Size([1, 47, 768])\n",
      "tok_embeds.shape torch.Size([1, 48, 768])\n",
      "tok_embeds.shape torch.Size([1, 49, 768])\n",
      "tok_embeds.shape torch.Size([1, 50, 768])\n",
      "tok_embeds.shape torch.Size([1, 51, 768])\n",
      "tok_embeds.shape torch.Size([1, 52, 768])\n",
      "tok_embeds.shape torch.Size([1, 53, 768])\n",
      "Every effort moves you, and,, and,,,,,,, and,.                                   \n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 3 (Step 000020): Train loss 5.589, Val loss 6.480\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 3 (Step 000025): Train loss 5.550, Val loss 6.412\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([1, 4, 768])\n",
      "tok_embeds.shape torch.Size([1, 5, 768])\n",
      "tok_embeds.shape torch.Size([1, 6, 768])\n",
      "tok_embeds.shape torch.Size([1, 7, 768])\n",
      "tok_embeds.shape torch.Size([1, 8, 768])\n",
      "tok_embeds.shape torch.Size([1, 9, 768])\n",
      "tok_embeds.shape torch.Size([1, 10, 768])\n",
      "tok_embeds.shape torch.Size([1, 11, 768])\n",
      "tok_embeds.shape torch.Size([1, 12, 768])\n",
      "tok_embeds.shape torch.Size([1, 13, 768])\n",
      "tok_embeds.shape torch.Size([1, 14, 768])\n",
      "tok_embeds.shape torch.Size([1, 15, 768])\n",
      "tok_embeds.shape torch.Size([1, 16, 768])\n",
      "tok_embeds.shape torch.Size([1, 17, 768])\n",
      "tok_embeds.shape torch.Size([1, 18, 768])\n",
      "tok_embeds.shape torch.Size([1, 19, 768])\n",
      "tok_embeds.shape torch.Size([1, 20, 768])\n",
      "tok_embeds.shape torch.Size([1, 21, 768])\n",
      "tok_embeds.shape torch.Size([1, 22, 768])\n",
      "tok_embeds.shape torch.Size([1, 23, 768])\n",
      "tok_embeds.shape torch.Size([1, 24, 768])\n",
      "tok_embeds.shape torch.Size([1, 25, 768])\n",
      "tok_embeds.shape torch.Size([1, 26, 768])\n",
      "tok_embeds.shape torch.Size([1, 27, 768])\n",
      "tok_embeds.shape torch.Size([1, 28, 768])\n",
      "tok_embeds.shape torch.Size([1, 29, 768])\n",
      "tok_embeds.shape torch.Size([1, 30, 768])\n",
      "tok_embeds.shape torch.Size([1, 31, 768])\n",
      "tok_embeds.shape torch.Size([1, 32, 768])\n",
      "tok_embeds.shape torch.Size([1, 33, 768])\n",
      "tok_embeds.shape torch.Size([1, 34, 768])\n",
      "tok_embeds.shape torch.Size([1, 35, 768])\n",
      "tok_embeds.shape torch.Size([1, 36, 768])\n",
      "tok_embeds.shape torch.Size([1, 37, 768])\n",
      "tok_embeds.shape torch.Size([1, 38, 768])\n",
      "tok_embeds.shape torch.Size([1, 39, 768])\n",
      "tok_embeds.shape torch.Size([1, 40, 768])\n",
      "tok_embeds.shape torch.Size([1, 41, 768])\n",
      "tok_embeds.shape torch.Size([1, 42, 768])\n",
      "tok_embeds.shape torch.Size([1, 43, 768])\n",
      "tok_embeds.shape torch.Size([1, 44, 768])\n",
      "tok_embeds.shape torch.Size([1, 45, 768])\n",
      "tok_embeds.shape torch.Size([1, 46, 768])\n",
      "tok_embeds.shape torch.Size([1, 47, 768])\n",
      "tok_embeds.shape torch.Size([1, 48, 768])\n",
      "tok_embeds.shape torch.Size([1, 49, 768])\n",
      "tok_embeds.shape torch.Size([1, 50, 768])\n",
      "tok_embeds.shape torch.Size([1, 51, 768])\n",
      "tok_embeds.shape torch.Size([1, 52, 768])\n",
      "tok_embeds.shape torch.Size([1, 53, 768])\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 4 (Step 000030): Train loss 5.169, Val loss 6.369\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 4 (Step 000035): Train loss 4.987, Val loss 6.377\n",
      "tok_embeds.shape torch.Size([1, 4, 768])\n",
      "tok_embeds.shape torch.Size([1, 5, 768])\n",
      "tok_embeds.shape torch.Size([1, 6, 768])\n",
      "tok_embeds.shape torch.Size([1, 7, 768])\n",
      "tok_embeds.shape torch.Size([1, 8, 768])\n",
      "tok_embeds.shape torch.Size([1, 9, 768])\n",
      "tok_embeds.shape torch.Size([1, 10, 768])\n",
      "tok_embeds.shape torch.Size([1, 11, 768])\n",
      "tok_embeds.shape torch.Size([1, 12, 768])\n",
      "tok_embeds.shape torch.Size([1, 13, 768])\n",
      "tok_embeds.shape torch.Size([1, 14, 768])\n",
      "tok_embeds.shape torch.Size([1, 15, 768])\n",
      "tok_embeds.shape torch.Size([1, 16, 768])\n",
      "tok_embeds.shape torch.Size([1, 17, 768])\n",
      "tok_embeds.shape torch.Size([1, 18, 768])\n",
      "tok_embeds.shape torch.Size([1, 19, 768])\n",
      "tok_embeds.shape torch.Size([1, 20, 768])\n",
      "tok_embeds.shape torch.Size([1, 21, 768])\n",
      "tok_embeds.shape torch.Size([1, 22, 768])\n",
      "tok_embeds.shape torch.Size([1, 23, 768])\n",
      "tok_embeds.shape torch.Size([1, 24, 768])\n",
      "tok_embeds.shape torch.Size([1, 25, 768])\n",
      "tok_embeds.shape torch.Size([1, 26, 768])\n",
      "tok_embeds.shape torch.Size([1, 27, 768])\n",
      "tok_embeds.shape torch.Size([1, 28, 768])\n",
      "tok_embeds.shape torch.Size([1, 29, 768])\n",
      "tok_embeds.shape torch.Size([1, 30, 768])\n",
      "tok_embeds.shape torch.Size([1, 31, 768])\n",
      "tok_embeds.shape torch.Size([1, 32, 768])\n",
      "tok_embeds.shape torch.Size([1, 33, 768])\n",
      "tok_embeds.shape torch.Size([1, 34, 768])\n",
      "tok_embeds.shape torch.Size([1, 35, 768])\n",
      "tok_embeds.shape torch.Size([1, 36, 768])\n",
      "tok_embeds.shape torch.Size([1, 37, 768])\n",
      "tok_embeds.shape torch.Size([1, 38, 768])\n",
      "tok_embeds.shape torch.Size([1, 39, 768])\n",
      "tok_embeds.shape torch.Size([1, 40, 768])\n",
      "tok_embeds.shape torch.Size([1, 41, 768])\n",
      "tok_embeds.shape torch.Size([1, 42, 768])\n",
      "tok_embeds.shape torch.Size([1, 43, 768])\n",
      "tok_embeds.shape torch.Size([1, 44, 768])\n",
      "tok_embeds.shape torch.Size([1, 45, 768])\n",
      "tok_embeds.shape torch.Size([1, 46, 768])\n",
      "tok_embeds.shape torch.Size([1, 47, 768])\n",
      "tok_embeds.shape torch.Size([1, 48, 768])\n",
      "tok_embeds.shape torch.Size([1, 49, 768])\n",
      "tok_embeds.shape torch.Size([1, 50, 768])\n",
      "tok_embeds.shape torch.Size([1, 51, 768])\n",
      "tok_embeds.shape torch.Size([1, 52, 768])\n",
      "tok_embeds.shape torch.Size([1, 53, 768])\n",
      "Every effort moves you a a him, and the of the picture to the picture. Gisburn, and a was, and the of the of the of the a. I had been. I had been the of the of the a of the. I had been\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 5 (Step 000040): Train loss 4.326, Val loss 6.256\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([1, 4, 768])\n",
      "tok_embeds.shape torch.Size([1, 5, 768])\n",
      "tok_embeds.shape torch.Size([1, 6, 768])\n",
      "tok_embeds.shape torch.Size([1, 7, 768])\n",
      "tok_embeds.shape torch.Size([1, 8, 768])\n",
      "tok_embeds.shape torch.Size([1, 9, 768])\n",
      "tok_embeds.shape torch.Size([1, 10, 768])\n",
      "tok_embeds.shape torch.Size([1, 11, 768])\n",
      "tok_embeds.shape torch.Size([1, 12, 768])\n",
      "tok_embeds.shape torch.Size([1, 13, 768])\n",
      "tok_embeds.shape torch.Size([1, 14, 768])\n",
      "tok_embeds.shape torch.Size([1, 15, 768])\n",
      "tok_embeds.shape torch.Size([1, 16, 768])\n",
      "tok_embeds.shape torch.Size([1, 17, 768])\n",
      "tok_embeds.shape torch.Size([1, 18, 768])\n",
      "tok_embeds.shape torch.Size([1, 19, 768])\n",
      "tok_embeds.shape torch.Size([1, 20, 768])\n",
      "tok_embeds.shape torch.Size([1, 21, 768])\n",
      "tok_embeds.shape torch.Size([1, 22, 768])\n",
      "tok_embeds.shape torch.Size([1, 23, 768])\n",
      "tok_embeds.shape torch.Size([1, 24, 768])\n",
      "tok_embeds.shape torch.Size([1, 25, 768])\n",
      "tok_embeds.shape torch.Size([1, 26, 768])\n",
      "tok_embeds.shape torch.Size([1, 27, 768])\n",
      "tok_embeds.shape torch.Size([1, 28, 768])\n",
      "tok_embeds.shape torch.Size([1, 29, 768])\n",
      "tok_embeds.shape torch.Size([1, 30, 768])\n",
      "tok_embeds.shape torch.Size([1, 31, 768])\n",
      "tok_embeds.shape torch.Size([1, 32, 768])\n",
      "tok_embeds.shape torch.Size([1, 33, 768])\n",
      "tok_embeds.shape torch.Size([1, 34, 768])\n",
      "tok_embeds.shape torch.Size([1, 35, 768])\n",
      "tok_embeds.shape torch.Size([1, 36, 768])\n",
      "tok_embeds.shape torch.Size([1, 37, 768])\n",
      "tok_embeds.shape torch.Size([1, 38, 768])\n",
      "tok_embeds.shape torch.Size([1, 39, 768])\n",
      "tok_embeds.shape torch.Size([1, 40, 768])\n",
      "tok_embeds.shape torch.Size([1, 41, 768])\n",
      "tok_embeds.shape torch.Size([1, 42, 768])\n",
      "tok_embeds.shape torch.Size([1, 43, 768])\n",
      "tok_embeds.shape torch.Size([1, 44, 768])\n",
      "tok_embeds.shape torch.Size([1, 45, 768])\n",
      "tok_embeds.shape torch.Size([1, 46, 768])\n",
      "tok_embeds.shape torch.Size([1, 47, 768])\n",
      "tok_embeds.shape torch.Size([1, 48, 768])\n",
      "tok_embeds.shape torch.Size([1, 49, 768])\n",
      "tok_embeds.shape torch.Size([1, 50, 768])\n",
      "tok_embeds.shape torch.Size([1, 51, 768])\n",
      "tok_embeds.shape torch.Size([1, 52, 768])\n",
      "tok_embeds.shape torch.Size([1, 53, 768])\n",
      "Every effort moves you, I had been, I had been, I had been. Gisburn, I had been, I had been, in the of the of the of the of the of the of the honour of the of the man of the of the of\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 6 (Step 000045): Train loss 4.063, Val loss 6.266\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 6 (Step 000050): Train loss 3.583, Val loss 6.209\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([1, 4, 768])\n",
      "tok_embeds.shape torch.Size([1, 5, 768])\n",
      "tok_embeds.shape torch.Size([1, 6, 768])\n",
      "tok_embeds.shape torch.Size([1, 7, 768])\n",
      "tok_embeds.shape torch.Size([1, 8, 768])\n",
      "tok_embeds.shape torch.Size([1, 9, 768])\n",
      "tok_embeds.shape torch.Size([1, 10, 768])\n",
      "tok_embeds.shape torch.Size([1, 11, 768])\n",
      "tok_embeds.shape torch.Size([1, 12, 768])\n",
      "tok_embeds.shape torch.Size([1, 13, 768])\n",
      "tok_embeds.shape torch.Size([1, 14, 768])\n",
      "tok_embeds.shape torch.Size([1, 15, 768])\n",
      "tok_embeds.shape torch.Size([1, 16, 768])\n",
      "tok_embeds.shape torch.Size([1, 17, 768])\n",
      "tok_embeds.shape torch.Size([1, 18, 768])\n",
      "tok_embeds.shape torch.Size([1, 19, 768])\n",
      "tok_embeds.shape torch.Size([1, 20, 768])\n",
      "tok_embeds.shape torch.Size([1, 21, 768])\n",
      "tok_embeds.shape torch.Size([1, 22, 768])\n",
      "tok_embeds.shape torch.Size([1, 23, 768])\n",
      "tok_embeds.shape torch.Size([1, 24, 768])\n",
      "tok_embeds.shape torch.Size([1, 25, 768])\n",
      "tok_embeds.shape torch.Size([1, 26, 768])\n",
      "tok_embeds.shape torch.Size([1, 27, 768])\n",
      "tok_embeds.shape torch.Size([1, 28, 768])\n",
      "tok_embeds.shape torch.Size([1, 29, 768])\n",
      "tok_embeds.shape torch.Size([1, 30, 768])\n",
      "tok_embeds.shape torch.Size([1, 31, 768])\n",
      "tok_embeds.shape torch.Size([1, 32, 768])\n",
      "tok_embeds.shape torch.Size([1, 33, 768])\n",
      "tok_embeds.shape torch.Size([1, 34, 768])\n",
      "tok_embeds.shape torch.Size([1, 35, 768])\n",
      "tok_embeds.shape torch.Size([1, 36, 768])\n",
      "tok_embeds.shape torch.Size([1, 37, 768])\n",
      "tok_embeds.shape torch.Size([1, 38, 768])\n",
      "tok_embeds.shape torch.Size([1, 39, 768])\n",
      "tok_embeds.shape torch.Size([1, 40, 768])\n",
      "tok_embeds.shape torch.Size([1, 41, 768])\n",
      "tok_embeds.shape torch.Size([1, 42, 768])\n",
      "tok_embeds.shape torch.Size([1, 43, 768])\n",
      "tok_embeds.shape torch.Size([1, 44, 768])\n",
      "tok_embeds.shape torch.Size([1, 45, 768])\n",
      "tok_embeds.shape torch.Size([1, 46, 768])\n",
      "tok_embeds.shape torch.Size([1, 47, 768])\n",
      "tok_embeds.shape torch.Size([1, 48, 768])\n",
      "tok_embeds.shape torch.Size([1, 49, 768])\n",
      "tok_embeds.shape torch.Size([1, 50, 768])\n",
      "tok_embeds.shape torch.Size([1, 51, 768])\n",
      "tok_embeds.shape torch.Size([1, 52, 768])\n",
      "tok_embeds.shape torch.Size([1, 53, 768])\n",
      "Every effort moves you know the                                                \n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 7 (Step 000055): Train loss 3.621, Val loss 6.183\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 7 (Step 000060): Train loss 2.820, Val loss 6.153\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([1, 4, 768])\n",
      "tok_embeds.shape torch.Size([1, 5, 768])\n",
      "tok_embeds.shape torch.Size([1, 6, 768])\n",
      "tok_embeds.shape torch.Size([1, 7, 768])\n",
      "tok_embeds.shape torch.Size([1, 8, 768])\n",
      "tok_embeds.shape torch.Size([1, 9, 768])\n",
      "tok_embeds.shape torch.Size([1, 10, 768])\n",
      "tok_embeds.shape torch.Size([1, 11, 768])\n",
      "tok_embeds.shape torch.Size([1, 12, 768])\n",
      "tok_embeds.shape torch.Size([1, 13, 768])\n",
      "tok_embeds.shape torch.Size([1, 14, 768])\n",
      "tok_embeds.shape torch.Size([1, 15, 768])\n",
      "tok_embeds.shape torch.Size([1, 16, 768])\n",
      "tok_embeds.shape torch.Size([1, 17, 768])\n",
      "tok_embeds.shape torch.Size([1, 18, 768])\n",
      "tok_embeds.shape torch.Size([1, 19, 768])\n",
      "tok_embeds.shape torch.Size([1, 20, 768])\n",
      "tok_embeds.shape torch.Size([1, 21, 768])\n",
      "tok_embeds.shape torch.Size([1, 22, 768])\n",
      "tok_embeds.shape torch.Size([1, 23, 768])\n",
      "tok_embeds.shape torch.Size([1, 24, 768])\n",
      "tok_embeds.shape torch.Size([1, 25, 768])\n",
      "tok_embeds.shape torch.Size([1, 26, 768])\n",
      "tok_embeds.shape torch.Size([1, 27, 768])\n",
      "tok_embeds.shape torch.Size([1, 28, 768])\n",
      "tok_embeds.shape torch.Size([1, 29, 768])\n",
      "tok_embeds.shape torch.Size([1, 30, 768])\n",
      "tok_embeds.shape torch.Size([1, 31, 768])\n",
      "tok_embeds.shape torch.Size([1, 32, 768])\n",
      "tok_embeds.shape torch.Size([1, 33, 768])\n",
      "tok_embeds.shape torch.Size([1, 34, 768])\n",
      "tok_embeds.shape torch.Size([1, 35, 768])\n",
      "tok_embeds.shape torch.Size([1, 36, 768])\n",
      "tok_embeds.shape torch.Size([1, 37, 768])\n",
      "tok_embeds.shape torch.Size([1, 38, 768])\n",
      "tok_embeds.shape torch.Size([1, 39, 768])\n",
      "tok_embeds.shape torch.Size([1, 40, 768])\n",
      "tok_embeds.shape torch.Size([1, 41, 768])\n",
      "tok_embeds.shape torch.Size([1, 42, 768])\n",
      "tok_embeds.shape torch.Size([1, 43, 768])\n",
      "tok_embeds.shape torch.Size([1, 44, 768])\n",
      "tok_embeds.shape torch.Size([1, 45, 768])\n",
      "tok_embeds.shape torch.Size([1, 46, 768])\n",
      "tok_embeds.shape torch.Size([1, 47, 768])\n",
      "tok_embeds.shape torch.Size([1, 48, 768])\n",
      "tok_embeds.shape torch.Size([1, 49, 768])\n",
      "tok_embeds.shape torch.Size([1, 50, 768])\n",
      "tok_embeds.shape torch.Size([1, 51, 768])\n",
      "tok_embeds.shape torch.Size([1, 52, 768])\n",
      "tok_embeds.shape torch.Size([1, 53, 768])\n",
      "Every effort moves you know the picture to see the picture.                    \"I he was his pictures-c.             \n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 8 (Step 000065): Train loss 2.388, Val loss 6.143\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 8 (Step 000070): Train loss 2.031, Val loss 6.192\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([1, 4, 768])\n",
      "tok_embeds.shape torch.Size([1, 5, 768])\n",
      "tok_embeds.shape torch.Size([1, 6, 768])\n",
      "tok_embeds.shape torch.Size([1, 7, 768])\n",
      "tok_embeds.shape torch.Size([1, 8, 768])\n",
      "tok_embeds.shape torch.Size([1, 9, 768])\n",
      "tok_embeds.shape torch.Size([1, 10, 768])\n",
      "tok_embeds.shape torch.Size([1, 11, 768])\n",
      "tok_embeds.shape torch.Size([1, 12, 768])\n",
      "tok_embeds.shape torch.Size([1, 13, 768])\n",
      "tok_embeds.shape torch.Size([1, 14, 768])\n",
      "tok_embeds.shape torch.Size([1, 15, 768])\n",
      "tok_embeds.shape torch.Size([1, 16, 768])\n",
      "tok_embeds.shape torch.Size([1, 17, 768])\n",
      "tok_embeds.shape torch.Size([1, 18, 768])\n",
      "tok_embeds.shape torch.Size([1, 19, 768])\n",
      "tok_embeds.shape torch.Size([1, 20, 768])\n",
      "tok_embeds.shape torch.Size([1, 21, 768])\n",
      "tok_embeds.shape torch.Size([1, 22, 768])\n",
      "tok_embeds.shape torch.Size([1, 23, 768])\n",
      "tok_embeds.shape torch.Size([1, 24, 768])\n",
      "tok_embeds.shape torch.Size([1, 25, 768])\n",
      "tok_embeds.shape torch.Size([1, 26, 768])\n",
      "tok_embeds.shape torch.Size([1, 27, 768])\n",
      "tok_embeds.shape torch.Size([1, 28, 768])\n",
      "tok_embeds.shape torch.Size([1, 29, 768])\n",
      "tok_embeds.shape torch.Size([1, 30, 768])\n",
      "tok_embeds.shape torch.Size([1, 31, 768])\n",
      "tok_embeds.shape torch.Size([1, 32, 768])\n",
      "tok_embeds.shape torch.Size([1, 33, 768])\n",
      "tok_embeds.shape torch.Size([1, 34, 768])\n",
      "tok_embeds.shape torch.Size([1, 35, 768])\n",
      "tok_embeds.shape torch.Size([1, 36, 768])\n",
      "tok_embeds.shape torch.Size([1, 37, 768])\n",
      "tok_embeds.shape torch.Size([1, 38, 768])\n",
      "tok_embeds.shape torch.Size([1, 39, 768])\n",
      "tok_embeds.shape torch.Size([1, 40, 768])\n",
      "tok_embeds.shape torch.Size([1, 41, 768])\n",
      "tok_embeds.shape torch.Size([1, 42, 768])\n",
      "tok_embeds.shape torch.Size([1, 43, 768])\n",
      "tok_embeds.shape torch.Size([1, 44, 768])\n",
      "tok_embeds.shape torch.Size([1, 45, 768])\n",
      "tok_embeds.shape torch.Size([1, 46, 768])\n",
      "tok_embeds.shape torch.Size([1, 47, 768])\n",
      "tok_embeds.shape torch.Size([1, 48, 768])\n",
      "tok_embeds.shape torch.Size([1, 49, 768])\n",
      "tok_embeds.shape torch.Size([1, 50, 768])\n",
      "tok_embeds.shape torch.Size([1, 51, 768])\n",
      "tok_embeds.shape torch.Size([1, 52, 768])\n",
      "tok_embeds.shape torch.Size([1, 53, 768])\n",
      "Every effort moves you know,\" was, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you of the moment--as Jack himself, as he was his own of Jack's \"There were, in his\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 9 (Step 000075): Train loss 1.664, Val loss 6.221\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 9 (Step 000080): Train loss 1.317, Val loss 6.262\n",
      "tok_embeds.shape torch.Size([1, 4, 768])\n",
      "tok_embeds.shape torch.Size([1, 5, 768])\n",
      "tok_embeds.shape torch.Size([1, 6, 768])\n",
      "tok_embeds.shape torch.Size([1, 7, 768])\n",
      "tok_embeds.shape torch.Size([1, 8, 768])\n",
      "tok_embeds.shape torch.Size([1, 9, 768])\n",
      "tok_embeds.shape torch.Size([1, 10, 768])\n",
      "tok_embeds.shape torch.Size([1, 11, 768])\n",
      "tok_embeds.shape torch.Size([1, 12, 768])\n",
      "tok_embeds.shape torch.Size([1, 13, 768])\n",
      "tok_embeds.shape torch.Size([1, 14, 768])\n",
      "tok_embeds.shape torch.Size([1, 15, 768])\n",
      "tok_embeds.shape torch.Size([1, 16, 768])\n",
      "tok_embeds.shape torch.Size([1, 17, 768])\n",
      "tok_embeds.shape torch.Size([1, 18, 768])\n",
      "tok_embeds.shape torch.Size([1, 19, 768])\n",
      "tok_embeds.shape torch.Size([1, 20, 768])\n",
      "tok_embeds.shape torch.Size([1, 21, 768])\n",
      "tok_embeds.shape torch.Size([1, 22, 768])\n",
      "tok_embeds.shape torch.Size([1, 23, 768])\n",
      "tok_embeds.shape torch.Size([1, 24, 768])\n",
      "tok_embeds.shape torch.Size([1, 25, 768])\n",
      "tok_embeds.shape torch.Size([1, 26, 768])\n",
      "tok_embeds.shape torch.Size([1, 27, 768])\n",
      "tok_embeds.shape torch.Size([1, 28, 768])\n",
      "tok_embeds.shape torch.Size([1, 29, 768])\n",
      "tok_embeds.shape torch.Size([1, 30, 768])\n",
      "tok_embeds.shape torch.Size([1, 31, 768])\n",
      "tok_embeds.shape torch.Size([1, 32, 768])\n",
      "tok_embeds.shape torch.Size([1, 33, 768])\n",
      "tok_embeds.shape torch.Size([1, 34, 768])\n",
      "tok_embeds.shape torch.Size([1, 35, 768])\n",
      "tok_embeds.shape torch.Size([1, 36, 768])\n",
      "tok_embeds.shape torch.Size([1, 37, 768])\n",
      "tok_embeds.shape torch.Size([1, 38, 768])\n",
      "tok_embeds.shape torch.Size([1, 39, 768])\n",
      "tok_embeds.shape torch.Size([1, 40, 768])\n",
      "tok_embeds.shape torch.Size([1, 41, 768])\n",
      "tok_embeds.shape torch.Size([1, 42, 768])\n",
      "tok_embeds.shape torch.Size([1, 43, 768])\n",
      "tok_embeds.shape torch.Size([1, 44, 768])\n",
      "tok_embeds.shape torch.Size([1, 45, 768])\n",
      "tok_embeds.shape torch.Size([1, 46, 768])\n",
      "tok_embeds.shape torch.Size([1, 47, 768])\n",
      "tok_embeds.shape torch.Size([1, 48, 768])\n",
      "tok_embeds.shape torch.Size([1, 49, 768])\n",
      "tok_embeds.shape torch.Size([1, 50, 768])\n",
      "tok_embeds.shape torch.Size([1, 51, 768])\n",
      "tok_embeds.shape torch.Size([1, 52, 768])\n",
      "tok_embeds.shape torch.Size([1, 53, 768])\n",
      "Every effort moves you know,\" was not that my hostess was \"interesting\": on the last word.    \"!  \"Oh, I felt him back his head to the donkey--and I saw that, and down the room, I had\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 5\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "num_batches 1\n",
      "Ep 10 (Step 000085): Train loss 1.022, Val loss 6.291\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([2, 256, 768])\n",
      "tok_embeds.shape torch.Size([1, 4, 768])\n",
      "tok_embeds.shape torch.Size([1, 5, 768])\n",
      "tok_embeds.shape torch.Size([1, 6, 768])\n",
      "tok_embeds.shape torch.Size([1, 7, 768])\n",
      "tok_embeds.shape torch.Size([1, 8, 768])\n",
      "tok_embeds.shape torch.Size([1, 9, 768])\n",
      "tok_embeds.shape torch.Size([1, 10, 768])\n",
      "tok_embeds.shape torch.Size([1, 11, 768])\n",
      "tok_embeds.shape torch.Size([1, 12, 768])\n",
      "tok_embeds.shape torch.Size([1, 13, 768])\n",
      "tok_embeds.shape torch.Size([1, 14, 768])\n",
      "tok_embeds.shape torch.Size([1, 15, 768])\n",
      "tok_embeds.shape torch.Size([1, 16, 768])\n",
      "tok_embeds.shape torch.Size([1, 17, 768])\n",
      "tok_embeds.shape torch.Size([1, 18, 768])\n",
      "tok_embeds.shape torch.Size([1, 19, 768])\n",
      "tok_embeds.shape torch.Size([1, 20, 768])\n",
      "tok_embeds.shape torch.Size([1, 21, 768])\n",
      "tok_embeds.shape torch.Size([1, 22, 768])\n",
      "tok_embeds.shape torch.Size([1, 23, 768])\n",
      "tok_embeds.shape torch.Size([1, 24, 768])\n",
      "tok_embeds.shape torch.Size([1, 25, 768])\n",
      "tok_embeds.shape torch.Size([1, 26, 768])\n",
      "tok_embeds.shape torch.Size([1, 27, 768])\n",
      "tok_embeds.shape torch.Size([1, 28, 768])\n",
      "tok_embeds.shape torch.Size([1, 29, 768])\n",
      "tok_embeds.shape torch.Size([1, 30, 768])\n",
      "tok_embeds.shape torch.Size([1, 31, 768])\n",
      "tok_embeds.shape torch.Size([1, 32, 768])\n",
      "tok_embeds.shape torch.Size([1, 33, 768])\n",
      "tok_embeds.shape torch.Size([1, 34, 768])\n",
      "tok_embeds.shape torch.Size([1, 35, 768])\n",
      "tok_embeds.shape torch.Size([1, 36, 768])\n",
      "tok_embeds.shape torch.Size([1, 37, 768])\n",
      "tok_embeds.shape torch.Size([1, 38, 768])\n",
      "tok_embeds.shape torch.Size([1, 39, 768])\n",
      "tok_embeds.shape torch.Size([1, 40, 768])\n",
      "tok_embeds.shape torch.Size([1, 41, 768])\n",
      "tok_embeds.shape torch.Size([1, 42, 768])\n",
      "tok_embeds.shape torch.Size([1, 43, 768])\n",
      "tok_embeds.shape torch.Size([1, 44, 768])\n",
      "tok_embeds.shape torch.Size([1, 45, 768])\n",
      "tok_embeds.shape torch.Size([1, 46, 768])\n",
      "tok_embeds.shape torch.Size([1, 47, 768])\n",
      "tok_embeds.shape torch.Size([1, 48, 768])\n",
      "tok_embeds.shape torch.Size([1, 49, 768])\n",
      "tok_embeds.shape torch.Size([1, 50, 768])\n",
      "tok_embeds.shape torch.Size([1, 51, 768])\n",
      "tok_embeds.shape torch.Size([1, 52, 768])\n",
      "tok_embeds.shape torch.Size([1, 53, 768])\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"    \"I must he had the head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ffdd9cb-be16-41dd-a52c-723234c65483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "464e6439-175a-47c9-acd2-ff6879480424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61906d09-5dea-458e-aef2-560321bd2cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2504579a-0d09-4f55-ada6-3a3d74a790f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "tqdm version: 4.66.4\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b9283-9712-4dcd-aac8-993806ff860a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
